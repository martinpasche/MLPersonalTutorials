{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXzsQniBNTY3"
   },
   "source": [
    "# Multilayer perceptron with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dmvPAbUNTY4"
   },
   "source": [
    "### Problem formulation\n",
    "\n",
    "Let $\\mathbf{x} \\in \\mathbb{R}^2$ denote the input vector and $y \\in \\{0,1\\}$ the corresponding label.\n",
    "\n",
    "We assume that there exist a function $f(\\cdot; \\boldsymbol\\theta): \\mathbb{R}^2 \\mapsto [0,1]$ parametrized by $\\boldsymbol\\theta$ such that:\n",
    "\n",
    "$$p(y=1|\\mathbf{x} ; \\theta) = f(\\mathbf{x}; \\boldsymbol\\theta) = \\hat{y}, \\qquad p(y=0|\\mathbf{x} ; \\theta) = 1- f(\\mathbf{x}; \\boldsymbol\\theta) = 1- \\hat{y}$$\n",
    "\n",
    "Let's first load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAekKXQvNTY5"
   },
   "outputs": [],
   "source": [
    "my_seed = 1\n",
    "import numpy as np\n",
    "np.random.seed(my_seed)\n",
    "import torch\n",
    "torch.manual_seed(my_seed)\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import plot_data, plot_decision_boundary\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iV5LwC7xNTY5",
    "outputId": "e81ad7e8-9f85-4bda-ca3e-bb65fe897ac3"
   },
   "outputs": [],
   "source": [
    "X, Y = make_moons(n_samples=2000, noise=0.1)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.20, random_state=my_seed)\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "Y_train_tensor = torch.from_numpy(Y_train).float()\n",
    "X_val_tensor = torch.from_numpy(X_val).float()\n",
    "Y_val_tensor = torch.from_numpy(Y_val).float()\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print(Y_train_tensor.shape)\n",
    "print(X_val_tensor.shape)\n",
    "print(Y_val_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "uP-GWN49NTY6",
    "outputId": "dc9eab61-20fa-451a-83c1-4d5dd4c7d4cf"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(1, 1, facecolor='#4B6EA9')\n",
    "ax.set_title('training data')\n",
    "plot_data(ax, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAG5ZkjrNTY6"
   },
   "source": [
    "### Model\n",
    "\n",
    "In the following cell, define a class `MyFirstMLP` that implements a simple multi-layer perceptron with one hidden layer. This class should inherits from [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module), the base class for all neural network modules in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15neV4uDNTY7"
   },
   "outputs": [],
   "source": [
    "class MyFirstMLP(nn.Module):\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASAqwfYMNTY7"
   },
   "source": [
    "We can instanciate this MLP, which creates a model with randomly initialized parameters. Let's have a look to the resulting decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "z5l_326iNTY7",
    "outputId": "188145ef-a92f-4561-dbb9-59dbfe5803fb"
   },
   "outputs": [],
   "source": [
    "my_nn = MyFirstMLP(input_dim=2, output_dim=1, hidden_dim=50)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, facecolor='#4B6EA9')\n",
    "plot_decision_boundary(ax, X_train_tensor, Y_train_tensor, my_nn, use_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDGinFg1NTY7"
   },
   "source": [
    "### Training\n",
    "\n",
    "Now we need to train the model from a labeled training dataset $\\mathcal{D} = \\{\\mathbf{x}_i, y_i\\}_{i=1}^N$. As seen during the lesson, we need to define a loss function:\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\ell(y_i, \\hat{y}_i = f(\\mathbf{x}_i; \\boldsymbol\\theta)).$$\n",
    "\n",
    "For binary classification we use the **binary cross-entropy** loss:\n",
    "\n",
    "$$ \\ell(y, \\hat{y}) = - (y \\ln(\\hat{y}) + (1-y)\\ln(1-\\hat{y})). $$\n",
    "\n",
    "To estimate the model parameters $\\boldsymbol\\theta$ we have to minimize the loss function $\\mathcal{L}(\\boldsymbol\\theta)$. To do so, we can use the [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) algorithm. It is an iterative algorithm which consists in iterating:\n",
    "\n",
    "$$ \\boldsymbol\\theta \\leftarrow \\boldsymbol\\theta - \\gamma \\nabla \\mathcal{L}(\\boldsymbol\\theta), $$\n",
    "\n",
    "where $\\gamma$ is the learning rate. Both the learning rate and the initialization of the parameters have a critical influence on the behavior of the algorithm.\n",
    "\n",
    "We have seen during the lesson that the gradient is computed using an algorithm called backpropagation. Fortunately, PyTorch handles this step automatically.\n",
    "\n",
    "In the following cell, implement the PyTorch pipeline to train the model `my_nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CSU6zYb9NTY8"
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "7d9Ob6j7NTY8",
    "outputId": "313a90c6-df86-4598-f6a4-cbae8f2f7139"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.legend([\"training\", \"validation\"])\n",
    "plt.title(\"loss\")\n",
    "plt.xlabel(\"epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "tA5D3E9ENTY8",
    "outputId": "9ca8dea8-7ab1-4213-9119-63badcf8bf0c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, facecolor='#4B6EA9')\n",
    "plot_decision_boundary(ax, X_train_tensor, Y_train_tensor, my_nn, use_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiuaARx9NTZA"
   },
   "source": [
    "## Questions\n",
    "\n",
    "- What results do you obtain if you remove the hidden layer? Why?\n",
    "- What results do you obtain if you add one or several hidden layers?\n",
    "- What happens if you choose a learning rate that is either too low or too high?\n",
    "- How can you use the validation set to choose the number of hidden layers and the number of training iterations?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MLP with PyTorch and from scratch (solution).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
