{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgement:** This notebook is inspired from https://github.com/glouppe/info8010-deep-learning/tree/master/tutorials/lecture_01/notebook\n",
    "\n",
    "# Polynomial Regression with PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "my_seed = 1\n",
    "import numpy as np\n",
    "np.random.seed(my_seed)\n",
    "import torch\n",
    "torch.manual_seed(my_seed)\n",
    "import torch.nn as nn\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "We have access to a dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$ where the pairs $(x_i, y_i)$ are assumed to be independent and identically distributed (i.i.d) according to an unknown distribution $p^\\star(x, y)$:\n",
    "\n",
    "$$ (x_i, y_i) \\overset{i.i.d}{\\sim} p^\\star(x, y).$$\n",
    "\n",
    "Actually, in this example, we are going to generate a synthetic dataset from a known distribution $p^\\star(x, y)$. But in general, when you collect data in the wild, keep in mind that **you do not know the true underlying data distribution**. \n",
    "\n",
    "The data are generated according to $ p^\\star(x, y) = p^\\star(y|x)p^\\star(x)$, where\n",
    "\n",
    "$$ p^\\star(x) = \\mathcal{U}([-20,20]); $$\n",
    "$$ p^\\star(y|x) = \\mathcal{N}(g(x), \\sigma^2),$$ \n",
    "\n",
    "with $g(x) = 0.1 (x-2)^3 + x^2 - 8x - 1$.\n",
    "\n",
    "In practice, we first sample $x$ from the uniform distribution $\\mathcal{U}([-20,20])$ and $\\epsilon$ from the Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$, then we compute $y = g(x) + \\epsilon$. We repeat this process independently $N$ times to build $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$.\n",
    "\n",
    "Let's generate and visualize our synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    # the true polynomial g(x)\n",
    "    return 0.1 * (x-2) ** 3 + x ** 2 - 8.0 * x - 1.0\n",
    "\n",
    "def generate(N):\n",
    "    # dataset generation\n",
    "    x = np.random.rand(N) * 40.0 - 20.0\n",
    "    y = g(x) + 50 * np.random.randn(N)\n",
    "    \n",
    "    return x.reshape(N, 1), y.reshape(N, 1)\n",
    "\n",
    "N = 50 # number of samples\n",
    "x_train, y_train = generate(N) # dataset\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(-20, 20, num=1000)\n",
    "plt.plot(xs, g(xs), c=\"r\", label=\"$g(x)$\")\n",
    "plt.scatter(x_train, y_train, label=\"$y = g(x) + \\epsilon$\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(color=\"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla linear neural network\n",
    "\n",
    "We want to find a function $f_\\theta: \\mathbb{R} \\mapsto \\mathbb{R}$ parameterized by $\\theta \\in \\mathbb{R}^P$ such that on average over $p^\\star(x,y)$, $y$ is well approximated by $f_\\theta(x)$.\n",
    "\n",
    "**Model definition:** Assume that we have no idea about the true data $p^\\star(x,y)$. A first thing to try is to fit a simple linear model. We thus define $f_\\theta(x) = w x + b$, where $\\theta = (w,b) \\in \\mathbb{R}^2$. \n",
    "\n",
    "This linear model is defined in the following cell, using the base class for all neural networks in Pytorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module), a linear layer [torch.nn.Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear), and an identity activation function, which is useless here but it is introduced for generality purposes (you should follow a similar template when using non-linear activation functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFirstNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MyFirstNN, self).__init__() # mandatory for all models based on torch.nn.Module\n",
    "        self.fc = nn.Linear(input_dim, output_dim) # we define a linear layer\n",
    "        self.act_fn = nn.Identity() # we define an identity activation function\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # This method is mandatory for all models based on torch.nn.Module. \n",
    "        # It defines the forward pass in the neural network, i.e. computing the output given the input\n",
    "        # as parameter. You have to use the layers and activation functions that were defined in the __init__ \n",
    "        # method.\n",
    "    \n",
    "        return self.act_fn(self.fc(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instantiate this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn1 = MyFirstNN(1, 1)\n",
    "\n",
    "print(my_nn1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss definition:** We also have to define a loss, in order to measure the quality of our prediction $\\hat{y} := f_\\theta(x)$ with respect to the ground truth $y$, for $(x,y) \\sim p^\\star(x,y)$. For this regression problem, we use the squared error:\n",
    "\n",
    "$$ \\mathcal{l}(y, f_\\theta(x)) = (y - f_\\theta(x))^2.$$\n",
    "\n",
    "Following the principle of empirical risk minimization, we want to estimate the model parameters $\\theta$ that minimize the average loss over the training dataset $\\mathcal{D}$. The empirical risk (i.e. the global loss to minimize) is defined by:\n",
    "\n",
    "$$ \\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{l}(y_i, f_\\theta(x_i)) = \\frac{1}{N} \\sum_{i=1}^N (y_i - f_\\theta(x_i))^2,$$\n",
    "\n",
    "which corresponds to the mean squared error. In the following cell, we define this loss with [torch.nn.MSELoss](https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minimization algorithm:** We use standard gradient descent to minimize $\\mathcal{L}(\\theta)$, which is implemented in [torch.optim.SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD). When instantiating this class, we need to provide the list of model parameters and the learning rate (see the ```train``` function below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training with PyTorch** We now have to train this model using our dataset. The following function gives you the basic PyTorch cooking recipe that you should follow to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, my_nn, loss_fn, optimizer, learning_rate, num_epoch):\n",
    "    \n",
    "    # We turn our model in train mode. Some layers that we do not use here \n",
    "    # behave differently on train and eval mode. \n",
    "    my_nn.train() \n",
    "    \n",
    "    # We define the optimizer\n",
    "    optimizer = optimizer(my_nn.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loss = []\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        my_nn.zero_grad() # reset the stored gradients for the parameters of the neural network\n",
    "\n",
    "        y_hat = my_nn(x) # do the forward pass\n",
    "\n",
    "        loss = loss_fn(y_hat, y) # compute the loss\n",
    "        # this is equivalent to:\n",
    "        # loss = (y_pred - y_train_tensor).pow(2).mean()    \n",
    "\n",
    "        loss.backward() # do the backward pass\n",
    "\n",
    "        optimizer.step() # do a SGD step\n",
    "        ## this is equivalent to:\n",
    "        # for p in my_nn1.parameters():\n",
    "        #    p.data = p.data - learning_rate*p.grad.data\n",
    "\n",
    "        train_loss.append(loss.item()) # store the loss\n",
    "    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input/output data to tensors\n",
    "x_train_tensor = torch.from_numpy(x_train).type(dtype)\n",
    "y_train_tensor = torch.from_numpy(y_train).type(dtype)\n",
    "\n",
    "# training\n",
    "train_loss = train(x=x_train_tensor, \n",
    "                   y=y_train_tensor, \n",
    "                   my_nn=my_nn1, \n",
    "                   loss_fn=loss_fn, \n",
    "                   optimizer=optimizer, \n",
    "                   learning_rate=1e-3, \n",
    "                   num_epoch=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(loss, my_nn):\n",
    "    \n",
    "    # plot loss\n",
    "    plt.figure()\n",
    "    plt.plot(loss)\n",
    "    plt.title('loss function')\n",
    "    plt.xlabel('epochs')\n",
    "    \n",
    "    # plot learned function\n",
    "    xs = np.linspace(-20, 20, num=1000)\n",
    "    xs_tensor = torch.from_numpy(xs).type(dtype).unsqueeze(1)\n",
    "    with torch.no_grad():\n",
    "        y_pred = my_nn(xs_tensor).numpy()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(xs, g(xs), c=\"r\", label=\"$g(x)$\")\n",
    "    plt.scatter(x_train, y_train, label=\"$y = g(x) + \\epsilon$\")\n",
    "    plt.plot(xs, y_pred, c=\"k\", label=\"$\\hat{g}(x)$\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(color=\"grey\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_results(train_loss, my_nn1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we obtain a very poor prediction (black curve). Indeed, we cannot fit a polynomial with a linear model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression with PyTorch\n",
    "\n",
    "To overcome the limitations of the previous linear model, define a polynomial model called ```MySecondNN``` (a class inheritating from [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module), similarly as before). Train it and visualize the results, similarly as before. \n",
    "\n",
    "You should be able to define the appropriate model by only using the following functions/classes:\n",
    "\n",
    "- [torch.nn.Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear)\n",
    "- [torch.pow](https://pytorch.org/docs/stable/torch.html#torch.pow) or `**`\n",
    "- [torch.cat](https://pytorch.org/docs/stable/torch.html?highlight=torch%20cat#torch.cat) \n",
    "\n",
    "Remember that the learning rate and the number of epochs are important parameters of the gradient descent algorithm..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySecondNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MySecondNN, self).__init__()\n",
    "        \n",
    "        # TO DO\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TO DO\n",
    "        \n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn2 = MySecondNN(1, 1)\n",
    "\n",
    "train_loss = train(x=x_train_tensor, \n",
    "                   y=y_train_tensor, \n",
    "                   my_nn=my_nn2, \n",
    "                   loss_fn=loss_fn, \n",
    "                   optimizer=optimizer, \n",
    "                   learning_rate=1e-8, \n",
    "                   num_epoch=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(train_loss, my_nn2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial fit with a multi-layer perceptron\n",
    "\n",
    "The previous model is polynomial with respect to the input, but **linear** with respect to the model parameters. We could actually fit a standard multilayer perceptron on our training samples. The resulting model would be **non-linear** with respect to the model parameters. This is what we do below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyThirdNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(MyThirdNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = nn.ReLU() # we use Rectified Linear Units activations\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn3 = MyThirdNN(1, 1, 50)\n",
    "\n",
    "# standard gradient descent gives poor results here, so we choose another optimization method called Adam\n",
    "optimizer = torch.optim.Adam \n",
    "\n",
    "train_loss = train(x=x_train_tensor, \n",
    "                   y=y_train_tensor, \n",
    "                   my_nn=my_nn3, \n",
    "                   loss_fn=loss_fn, \n",
    "                   optimizer=optimizer, \n",
    "                   learning_rate=1e-2, \n",
    "                   num_epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(train_loss, my_nn3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model is ok, but it is clearly not better, or maybe even worse than the previous one, which moreover has much less parameters. Simple linear models can sometimes do a better job than neural networks...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
