{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is adapted from [this one](https://github.com/dataflowr/notebooks/blob/archive-2020/Notebooks/02_basics_pytorch.ipynb) written by [Marc Lelarge](https://www.di.ens.fr/~lelarge/), and from the tutorials [What is PyTorch?](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#) and [Autograd: Automatic Differentiation](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py).\n",
    "\n",
    "You can have access to more tutorials [here](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).\n",
    "\n",
    "When you have questions, try as much as possible to find the answer by yourself looking at [PyTorch documentation](https://pytorch.org/docs/stable/index.html). You need to learn to work with the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Playing with PyTorch: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up: Linear regression with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a dataset $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$  where $\\mathbf{x}_i \\in \\mathbb{R}^2$ are the input features and $y_i \\in \\mathbb{R}$ the labels. The dataset is generated as follows:\n",
    "\n",
    "$$ y_i = 2x_{1,i}-3x_{2,i}+1, \\quad i\\in\\{1,\\dots,N\\},$$\n",
    "where $\\mathbf{x}_i = [x_{1,i}, x_{2,i}]^T$.\n",
    "\n",
    "We define the following model:\n",
    "$$ \\hat{y}_i = w_1 x_{1,i} + w_2 x_{2,i} + b, \\quad i\\in\\{1,\\dots,N\\}, $$\n",
    "\n",
    "or equivalently in vector form:\n",
    "$$ \\hat{y}_i = \\mathbf{w}^T\\mathbf{x}_i + b, $$\n",
    "where $\\mathbf{w} = [w_1, w_2]^T$.\n",
    "\n",
    "Our task is to recover the weights $w_1=2, w_2=-3$ and the bias $b = 1$ from the dataset $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import random\n",
    "import torch\n",
    "\n",
    "# generate random input data\n",
    "N = 30\n",
    "x = random((N,2))\n",
    "\n",
    "# generate labels corresponding to input data x\n",
    "w_source = np.array([2., -3.])\n",
    "b_source  = np.array([1.])\n",
    "\n",
    "y = x @ w_source + b_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some visualization functions that you do not need to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_figs(fig_num, elev, azim, x, y, weights, bias):\n",
    "    fig = plt.figure(fig_num, figsize=(4, 3))\n",
    "    plt.clf()\n",
    "    ax = Axes3D(fig, elev=elev, azim=azim)\n",
    "    ax.scatter(x[:, 0], x[:, 1], y)\n",
    "    X = np.array([[0, 0], [1, 1]])\n",
    "    Y = np.array([[0, 1], [0, 1]])\n",
    "    Z = (np.array([[0, 0, 1, 1], [0, 1, 0, 1]]).T @ weights + bias).reshape((2, 2))\n",
    "    # [Z]_ij = 2[X]_ij - 3[Y]_ij + 1\n",
    "    ax.plot_surface(X,Y,Z,alpha=.5)\n",
    "    ax.set_xlabel('x_1')\n",
    "    ax.set_ylabel('x_2')\n",
    "    ax.set_zlabel('y')\n",
    "    \n",
    "def plot_views(x, y, w, b):\n",
    "    #Generate the different figures from different views\n",
    "    elev = 10\n",
    "    azim = -110\n",
    "    plot_figs(1, elev, azim, x, y, w, b[0])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plot_views(x, y, w_source, b_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize the following loss function with respect to $\\mathbf{w}$ and $b$:\n",
    "$$ \\mathcal{L}(\\mathbf{w}, b) = \\frac{1}{N}\\sum\\limits_{i=1}^{N} \\ell_i(\\mathbf{w}, b), \\qquad \\text{with} \\qquad \\ell_i(\\mathbf{w}, b) = \\left(\\mathbf{w}^T{\\bf x}_i + b - y_i \\right)^2. $$\n",
    "\n",
    "We first compute the gradient of each term in this sum:\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial{\\ell_i}}{\\partial w_1} &=& 2x_{1,i}\\left({\\bf w}^T{\\bf x}_i+b-y_i \\right);\\\\\n",
    "\\frac{\\partial{\\ell_i}}{\\partial w_2} &=& 2x_{2,i}\\left({\\bf w}^T{\\bf x}_i+b-y_i \\right);\\\\\n",
    "\\frac{\\partial{\\ell_i}}{\\partial b} &=& 2\\left({\\bf w}^T{\\bf x}_i+b-y_i \\right).\n",
    "\\end{eqnarray*}\n",
    "\n",
    "For one epoch (one pass over the entire dataset), **stochastic gradient descent** updates the weigts and bias by running the following loop: \n",
    "\n",
    "for $i \\in \\{1,\\dots,N\\}$, \n",
    "\n",
    "\\begin{eqnarray*}\n",
    "w_1 &\\leftarrow& w_{1} - \\alpha\\frac{\\partial{\\ell_i}}{\\partial w_1}; \\\\\n",
    "w_{2} &\\leftarrow& w_{2} - \\alpha\\frac{\\partial{\\ell_i}}{\\partial w_2}; \\\\\n",
    "b &\\leftarrow & b - \\alpha\\frac{\\partial{\\ell_i}}{\\partial b};\n",
    "\\end{eqnarray*}\n",
    "where $\\alpha>0$ is called the learning rate.\n",
    "\n",
    "Then we run several epochs.\n",
    "\n",
    "Note that it is common to shuffle the dataset between two epochs, so that the order of the examples when iterating over $i \\in \\{1,\\dots,N\\}$ is not always the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize learnable weights and bias\n",
    "w_init = random(2)\n",
    "b_init = random(1)\n",
    "\n",
    "w = w_init\n",
    "b = b_init\n",
    "print(\"initial values of the parameters:\", w, b )\n",
    "\n",
    "# We plot the prediction with the randomly initialized parameters\n",
    "plt.figure(figsize=(7,5))\n",
    "plot_views(x, y, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model forward pass\n",
    "def forward(x):\n",
    "    return x @ w + b\n",
    "\n",
    "# Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y)**2 \n",
    "\n",
    "# compute gradient\n",
    "def gradient(x, y):  \n",
    "    # returns dloss/dw, dloss/db\n",
    "    return 2*(forward(x) - y)*x, 2*(forward(x) - y)\n",
    " \n",
    "learning_rate = 1e-2\n",
    "\n",
    "# Training loop with minibatch of size 1 (i.e., stochastic gradient descent)\n",
    "train_loss = []\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    l = 0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        grad_w, grad_b = gradient(x_i, y_i)\n",
    "        w = w - learning_rate * grad_w\n",
    "        b = b - learning_rate * grad_b\n",
    "        l += loss(x_i, y_i)\n",
    "    train_loss.append(l/y.shape[0])\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(train_loss)\n",
    "plt.xlabel('epochs')\n",
    "plt.title('training loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training\n",
    "print(\"estimation of the parameters:\", w, b )\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plot_views(x, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full gradients of the loss function are actually given by:\n",
    "$$ \\frac{\\partial{\\mathcal{L}}}{\\partial w_1} = \\frac{1}{N}\\sum\\limits_{i=1}^{N} \\frac{\\partial{\\ell_i}}{\\partial w_1},\\quad\n",
    "\\frac{\\partial{\\mathcal{L}}}{\\partial w_2} = \\frac{1}{N}\\sum\\limits_{i=1}^{N} \\frac{\\partial{\\ell_i}}{\\partial w_2},\\quad\n",
    "\\frac{\\partial{\\mathcal{L}}}{\\partial b} = \\frac{1}{N}\\sum\\limits_{i=1}^{N} \\frac{\\partial{\\ell_i}}{\\partial b}. $$\n",
    "\n",
    "For one epoch, **standard gradient descent** updates the weights and bias as follows:\n",
    "\\begin{eqnarray*}\n",
    "w_1 &\\leftarrow& w_{1} - \\alpha\\frac{\\partial{\\mathcal{L}}}{\\partial w_1}; \\\\\n",
    "w_{2} &\\leftarrow& w_{2} - \\alpha\\frac{\\partial{\\mathcal{L}}}{\\partial w_2}; \\\\\n",
    "b &\\leftarrow & b - \\alpha\\frac{\\partial{\\mathcal{L}}}{\\partial b};\n",
    "\\end{eqnarray*}\n",
    "\n",
    "and then we run several epochs. Standard gradient descent is implemented in the following cells. For this simple linear regression example, you should not see much difference between standard gradient descent and its stochastic version because the loss function is convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w_init\n",
    "b = b_init\n",
    "\n",
    "print(\"initial values of the parameters:\", w, b )\n",
    "\n",
    "learning_rate = 5e-1\n",
    "train_loss = []\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    grad_w = np.array([0.0,0.0])\n",
    "    grad_b = np.array([0.0])\n",
    "    l = 0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        grad_w += gradient(x_i, y_i)[0]\n",
    "        grad_b += gradient(x_i, y_i)[1]\n",
    "        l += loss(x_i, y_i)\n",
    "    grad_w /= N\n",
    "    grad_b /= N\n",
    "    w = w - learning_rate * grad_w\n",
    "    b = b - learning_rate * grad_b\n",
    "    train_loss.append(l/y.shape[0])\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.xlabel('epochs')\n",
    "plt.title('training loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# After training\n",
    "print(\"estimation of the parameters:\", w, b)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plot_views(x, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with PyTorch tensors and Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by converting our data into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
    "x_t = torch.from_numpy(x).type(dtype)\n",
    "y_t = torch.from_numpy(y).type(dtype).unsqueeze(1)\n",
    "print(x_t.shape)\n",
    "print(y_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation of **gradient descent** with tensors and without computing explicitly the gradient, using autograd instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w_init_t = torch.from_numpy(w_init).type(dtype)\n",
    "b_init_t = torch.from_numpy(b_init).type(dtype)\n",
    "\n",
    "w_v = w_init_t.clone().unsqueeze(1)\n",
    "w_v.requires_grad_(True)\n",
    "\n",
    "b_v = b_init_t.clone().unsqueeze(1)\n",
    "b_v.requires_grad_(True)\n",
    "\n",
    "print(\"initial values of the parameters:\", w_v.detach().numpy().squeeze(), b_v.detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-1\n",
    "train_loss = []\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    y_pred = x_t@ w_v + b_v\n",
    "    loss = torch.mean((y_pred - y_t)**2)\n",
    "    \n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Variables with requires_grad=True.\n",
    "    # After this call w.grad and b.grad will be Variables holding the gradient\n",
    "    # of the loss with respect to w and b respectively.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights using gradient descent. For this step we just want to mutate\n",
    "    # the values of w_v and b_v in-place; we don't want to build up a computational\n",
    "    # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "    # to prevent PyTorch from building a computational graph for the updates\n",
    "    with torch.no_grad():\n",
    "        w_v -= learning_rate * w_v.grad\n",
    "        b_v -= learning_rate * b_v.grad\n",
    "    \n",
    "    # Manually zero the gradients after updating weights\n",
    "    # otherwise gradients will be acumulated after each .backward()\n",
    "    w_v.grad.zero_()\n",
    "    b_v.grad.zero_()\n",
    "    \n",
    "    train_loss.append(loss.item()/y_t.shape[0])\n",
    "\n",
    "plt.plot(train_loss)\n",
    "plt.xlabel('epochs')\n",
    "plt.title('training loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training\n",
    "print(\"estimation of the parameters:\", w_v.detach().numpy().squeeze(), b_v.detach().numpy().squeeze())\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plot_views(x, y, w_v.detach().numpy(), b_v.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should obtain exactly the same result as standard gradient descent with numpy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
